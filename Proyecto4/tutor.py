# probar_tutor_fix.py

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import os
import warnings
warnings.filterwarnings("ignore")

print("="*70)
print("ğŸ¤– TUTOR DE ALGORITMOS")
print("="*70)

# Verificar modelo
if not os.path.exists("./tutor-algoritmos-final"):
    print("\nâŒ No se encontrÃ³ el modelo")
    exit()

# Determinar dispositivo
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"\nğŸ“± Usando dispositivo: {device}")

print("ğŸ“¦ Cargando modelo base...")
base_model = AutoModelForCausalLM.from_pretrained(
    "microsoft/phi-2",
    torch_dtype=torch.float16 if device == "cuda" else torch.float32,
    trust_remote_code=True
)

print("ğŸ“¦ Cargando adaptador LoRA...")
model = PeftModel.from_pretrained(base_model, "./tutor-algoritmos-final")

# CRÃTICO: Mover TODO el modelo al mismo dispositivo
print(f"ğŸ”„ Moviendo modelo a {device}...")
model = model.to(device)
model.eval()

print("ğŸ“¦ Cargando tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("./tutor-algoritmos-final", trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

print("âœ… Todo listo\n")
print("="*70)

def preguntar(pregunta):
    """Pregunta al tutor - TODOS los tensores en el mismo dispositivo"""
    prompt = f"<|user|>\n{pregunta}\n<|assistant|>\n"
    
    # Tokenizar
    inputs = tokenizer(
        prompt, 
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    
    # CRÃTICO: Mover TODOS los inputs al mismo dispositivo que el modelo
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Generar
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=250,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # Decodificar
    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extraer solo respuesta del asistente
    if "<|assistant|>" in respuesta_completa:
        respuesta = respuesta_completa.split("<|assistant|>")[-1]
        if "<|end|>" in respuesta:
            respuesta = respuesta.split("<|end|>")[0]
        respuesta = respuesta.strip()
    else:
        respuesta = respuesta_completa.strip()
    
    return respuesta

# ========================================
# PRUEBAS AUTOMÃTICAS
# ========================================
print("\nğŸ§ª PRUEBAS AUTOMÃTICAS\n")

preguntas_prueba = [
    "Â¿Para quÃ© sirve realmente un Ãrbol Rojo-Negro?"]

for i, pregunta in enumerate(preguntas_prueba, 1):
    print(f"\nğŸ“ Pregunta {i}/{len(preguntas_prueba)}: {pregunta}")
    print("-"*70)
    
    try:
        respuesta = preguntar(pregunta)
        print(respuesta)
        print("="*70)
    except Exception as e:
        print(f"âŒ Error: {e}")
        print("="*70)

# ========================================
# MODO INTERACTIVO
# ========================================
print("\n\nğŸ’¬ MODO INTERACTIVO")
print("Escribe 'salir' para terminar\n")

while True:
    try:
        pregunta_usuario = input("\nğŸ“ Tu pregunta: ").strip()
        
        if not pregunta_usuario:
            continue
            
        if pregunta_usuario.lower() in ['salir', 'exit', 'quit', 'q']:
            print("\nğŸ‘‹ Â¡Hasta luego!")
            break
        
        print("\nğŸ¤– Tutor:")
        print("-"*70)
        respuesta = preguntar(pregunta_usuario)
        print(respuesta)
        print("-"*70)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Â¡Hasta luego!")
        break
    except Exception as e:
        print(f"\nâŒ Error al procesar pregunta: {e}")

print("\nâœ… SesiÃ³n terminada")