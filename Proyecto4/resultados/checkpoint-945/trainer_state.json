{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 15.0,
  "eval_steps": 500,
  "global_step": 945,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.08,
      "grad_norm": 0.5694540739059448,
      "learning_rate": 0.00019915343915343915,
      "loss": 1.7283,
      "step": 5
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.7738885283470154,
      "learning_rate": 0.0001980952380952381,
      "loss": 1.5012,
      "step": 10
    },
    {
      "epoch": 0.24,
      "grad_norm": 1.1548020839691162,
      "learning_rate": 0.00019703703703703704,
      "loss": 0.8764,
      "step": 15
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.8356185555458069,
      "learning_rate": 0.000195978835978836,
      "loss": 0.7241,
      "step": 20
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.835412859916687,
      "learning_rate": 0.00019492063492063493,
      "loss": 0.604,
      "step": 25
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.30218181014060974,
      "learning_rate": 0.00019386243386243388,
      "loss": 0.2166,
      "step": 30
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.4622892737388611,
      "learning_rate": 0.00019280423280423282,
      "loss": 0.278,
      "step": 35
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.2056102156639099,
      "learning_rate": 0.00019174603174603176,
      "loss": 0.302,
      "step": 40
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.41823840141296387,
      "learning_rate": 0.00019068783068783068,
      "loss": 0.4628,
      "step": 45
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.3624172806739807,
      "learning_rate": 0.00018962962962962965,
      "loss": 0.0992,
      "step": 50
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.42991796135902405,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.2083,
      "step": 55
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.3686445951461792,
      "learning_rate": 0.00018751322751322754,
      "loss": 0.2984,
      "step": 60
    },
    {
      "epoch": 1.032,
      "grad_norm": 0.38736650347709656,
      "learning_rate": 0.00018645502645502646,
      "loss": 0.3509,
      "step": 65
    },
    {
      "epoch": 1.112,
      "grad_norm": 0.13793757557868958,
      "learning_rate": 0.0001853968253968254,
      "loss": 0.1805,
      "step": 70
    },
    {
      "epoch": 1.192,
      "grad_norm": 0.12122534215450287,
      "learning_rate": 0.00018433862433862435,
      "loss": 0.1887,
      "step": 75
    },
    {
      "epoch": 1.272,
      "grad_norm": 0.23706993460655212,
      "learning_rate": 0.0001832804232804233,
      "loss": 0.2575,
      "step": 80
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 0.3209175765514374,
      "learning_rate": 0.00018222222222222224,
      "loss": 0.2081,
      "step": 85
    },
    {
      "epoch": 1.432,
      "grad_norm": 0.4589845836162567,
      "learning_rate": 0.00018116402116402118,
      "loss": 0.2984,
      "step": 90
    },
    {
      "epoch": 1.512,
      "grad_norm": 0.5544706583023071,
      "learning_rate": 0.0001801058201058201,
      "loss": 0.4015,
      "step": 95
    },
    {
      "epoch": 1.592,
      "grad_norm": 0.4310077726840973,
      "learning_rate": 0.00017904761904761907,
      "loss": 0.3047,
      "step": 100
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 0.2678273916244507,
      "learning_rate": 0.000177989417989418,
      "loss": 0.2769,
      "step": 105
    },
    {
      "epoch": 1.752,
      "grad_norm": 0.27654948830604553,
      "learning_rate": 0.00017693121693121696,
      "loss": 0.2732,
      "step": 110
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 0.2996141016483307,
      "learning_rate": 0.00017587301587301588,
      "loss": 0.2907,
      "step": 115
    },
    {
      "epoch": 1.912,
      "grad_norm": 0.4700843393802643,
      "learning_rate": 0.00017481481481481482,
      "loss": 0.3556,
      "step": 120
    },
    {
      "epoch": 1.992,
      "grad_norm": 0.41533493995666504,
      "learning_rate": 0.00017375661375661376,
      "loss": 0.2015,
      "step": 125
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.36395108699798584,
      "learning_rate": 0.0001726984126984127,
      "loss": 0.2785,
      "step": 130
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.511939287185669,
      "learning_rate": 0.00017164021164021165,
      "loss": 0.3935,
      "step": 135
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.16701100766658783,
      "learning_rate": 0.0001705820105820106,
      "loss": 0.1978,
      "step": 140
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.5310570597648621,
      "learning_rate": 0.00016952380952380954,
      "loss": 0.328,
      "step": 145
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.24825818836688995,
      "learning_rate": 0.00016846560846560849,
      "loss": 0.1644,
      "step": 150
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.36250731348991394,
      "learning_rate": 0.0001674074074074074,
      "loss": 0.3509,
      "step": 155
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.2932165265083313,
      "learning_rate": 0.00016634920634920637,
      "loss": 0.307,
      "step": 160
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.4230453372001648,
      "learning_rate": 0.0001652910052910053,
      "loss": 0.1365,
      "step": 165
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.25783219933509827,
      "learning_rate": 0.00016423280423280424,
      "loss": 0.2246,
      "step": 170
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.15725970268249512,
      "learning_rate": 0.00016317460317460318,
      "loss": 0.3458,
      "step": 175
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.41025131940841675,
      "learning_rate": 0.00016211640211640212,
      "loss": 0.1624,
      "step": 180
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.44987377524375916,
      "learning_rate": 0.00016105820105820107,
      "loss": 0.2838,
      "step": 185
    },
    {
      "epoch": 3.016,
      "grad_norm": 0.36803513765335083,
      "learning_rate": 0.00016,
      "loss": 0.1626,
      "step": 190
    },
    {
      "epoch": 3.096,
      "grad_norm": 0.5054768919944763,
      "learning_rate": 0.00015894179894179896,
      "loss": 0.3199,
      "step": 195
    },
    {
      "epoch": 3.176,
      "grad_norm": 0.10227975994348526,
      "learning_rate": 0.0001578835978835979,
      "loss": 0.1645,
      "step": 200
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 0.14280501008033752,
      "learning_rate": 0.00015682539682539682,
      "loss": 0.1636,
      "step": 205
    },
    {
      "epoch": 3.336,
      "grad_norm": 0.5158596038818359,
      "learning_rate": 0.0001557671957671958,
      "loss": 0.323,
      "step": 210
    },
    {
      "epoch": 3.416,
      "grad_norm": 0.5638315081596375,
      "learning_rate": 0.0001547089947089947,
      "loss": 0.1693,
      "step": 215
    },
    {
      "epoch": 3.496,
      "grad_norm": 0.4646042287349701,
      "learning_rate": 0.00015365079365079368,
      "loss": 0.1902,
      "step": 220
    },
    {
      "epoch": 3.576,
      "grad_norm": 0.45106494426727295,
      "learning_rate": 0.0001525925925925926,
      "loss": 0.2258,
      "step": 225
    },
    {
      "epoch": 3.656,
      "grad_norm": 0.38894587755203247,
      "learning_rate": 0.00015153439153439154,
      "loss": 0.3071,
      "step": 230
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 0.2729112505912781,
      "learning_rate": 0.00015047619047619048,
      "loss": 0.208,
      "step": 235
    },
    {
      "epoch": 3.816,
      "grad_norm": 0.4693009853363037,
      "learning_rate": 0.00014941798941798943,
      "loss": 0.2965,
      "step": 240
    },
    {
      "epoch": 3.896,
      "grad_norm": 0.13063354790210724,
      "learning_rate": 0.00014835978835978837,
      "loss": 0.2327,
      "step": 245
    },
    {
      "epoch": 3.976,
      "grad_norm": 0.3892936706542969,
      "learning_rate": 0.00014730158730158732,
      "loss": 0.2326,
      "step": 250
    },
    {
      "epoch": 4.048,
      "grad_norm": 0.24606579542160034,
      "learning_rate": 0.00014624338624338626,
      "loss": 0.1699,
      "step": 255
    },
    {
      "epoch": 4.128,
      "grad_norm": 0.600126326084137,
      "learning_rate": 0.0001451851851851852,
      "loss": 0.229,
      "step": 260
    },
    {
      "epoch": 4.208,
      "grad_norm": 0.32774749398231506,
      "learning_rate": 0.00014412698412698412,
      "loss": 0.2035,
      "step": 265
    },
    {
      "epoch": 4.288,
      "grad_norm": 0.39384472370147705,
      "learning_rate": 0.0001430687830687831,
      "loss": 0.2192,
      "step": 270
    },
    {
      "epoch": 4.368,
      "grad_norm": 0.4841137230396271,
      "learning_rate": 0.000142010582010582,
      "loss": 0.2631,
      "step": 275
    },
    {
      "epoch": 4.448,
      "grad_norm": 0.26108935475349426,
      "learning_rate": 0.00014095238095238096,
      "loss": 0.2611,
      "step": 280
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 0.3153703212738037,
      "learning_rate": 0.0001398941798941799,
      "loss": 0.1156,
      "step": 285
    },
    {
      "epoch": 4.608,
      "grad_norm": 0.6469926238059998,
      "learning_rate": 0.00013883597883597885,
      "loss": 0.2561,
      "step": 290
    },
    {
      "epoch": 4.688,
      "grad_norm": 0.4956141710281372,
      "learning_rate": 0.0001377777777777778,
      "loss": 0.2779,
      "step": 295
    },
    {
      "epoch": 4.768,
      "grad_norm": 0.5860787034034729,
      "learning_rate": 0.00013671957671957673,
      "loss": 0.1773,
      "step": 300
    },
    {
      "epoch": 4.848,
      "grad_norm": 0.39201799035072327,
      "learning_rate": 0.00013566137566137568,
      "loss": 0.2374,
      "step": 305
    },
    {
      "epoch": 4.928,
      "grad_norm": 0.11134917289018631,
      "learning_rate": 0.00013460317460317462,
      "loss": 0.2367,
      "step": 310
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.14570051431655884,
      "learning_rate": 0.00013354497354497354,
      "loss": 0.1192,
      "step": 315
    },
    {
      "epoch": 5.08,
      "grad_norm": 0.38653281331062317,
      "learning_rate": 0.0001324867724867725,
      "loss": 0.2512,
      "step": 320
    },
    {
      "epoch": 5.16,
      "grad_norm": 0.3330029845237732,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.386,
      "step": 325
    },
    {
      "epoch": 5.24,
      "grad_norm": 0.3294517397880554,
      "learning_rate": 0.0001303703703703704,
      "loss": 0.1705,
      "step": 330
    },
    {
      "epoch": 5.32,
      "grad_norm": 0.2915552854537964,
      "learning_rate": 0.00012931216931216932,
      "loss": 0.1881,
      "step": 335
    },
    {
      "epoch": 5.4,
      "grad_norm": 0.35222187638282776,
      "learning_rate": 0.00012825396825396826,
      "loss": 0.1613,
      "step": 340
    },
    {
      "epoch": 5.48,
      "grad_norm": 0.3285875916481018,
      "learning_rate": 0.0001271957671957672,
      "loss": 0.2091,
      "step": 345
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 0.32900726795196533,
      "learning_rate": 0.00012613756613756615,
      "loss": 0.2493,
      "step": 350
    },
    {
      "epoch": 5.64,
      "grad_norm": 0.5664218068122864,
      "learning_rate": 0.0001250793650793651,
      "loss": 0.1289,
      "step": 355
    },
    {
      "epoch": 5.72,
      "grad_norm": 0.06746991723775864,
      "learning_rate": 0.00012402116402116404,
      "loss": 0.1825,
      "step": 360
    },
    {
      "epoch": 5.8,
      "grad_norm": 0.4557947516441345,
      "learning_rate": 0.00012296296296296296,
      "loss": 0.1045,
      "step": 365
    },
    {
      "epoch": 5.88,
      "grad_norm": 0.685030460357666,
      "learning_rate": 0.00012190476190476193,
      "loss": 0.268,
      "step": 370
    },
    {
      "epoch": 5.96,
      "grad_norm": 0.5019468069076538,
      "learning_rate": 0.00012084656084656086,
      "loss": 0.2149,
      "step": 375
    },
    {
      "epoch": 6.032,
      "grad_norm": 0.3897237181663513,
      "learning_rate": 0.00011978835978835979,
      "loss": 0.1112,
      "step": 380
    },
    {
      "epoch": 6.112,
      "grad_norm": 0.3530236482620239,
      "learning_rate": 0.00011873015873015873,
      "loss": 0.2076,
      "step": 385
    },
    {
      "epoch": 6.192,
      "grad_norm": 0.6738759279251099,
      "learning_rate": 0.00011767195767195766,
      "loss": 0.1804,
      "step": 390
    },
    {
      "epoch": 6.272,
      "grad_norm": 0.13009940087795258,
      "learning_rate": 0.00011661375661375662,
      "loss": 0.1233,
      "step": 395
    },
    {
      "epoch": 6.352,
      "grad_norm": 0.35274574160575867,
      "learning_rate": 0.00011555555555555555,
      "loss": 0.1244,
      "step": 400
    },
    {
      "epoch": 6.432,
      "grad_norm": 0.3701600432395935,
      "learning_rate": 0.0001144973544973545,
      "loss": 0.1304,
      "step": 405
    },
    {
      "epoch": 6.5120000000000005,
      "grad_norm": 0.32861053943634033,
      "learning_rate": 0.00011343915343915343,
      "loss": 0.2123,
      "step": 410
    },
    {
      "epoch": 6.592,
      "grad_norm": 0.1234162375330925,
      "learning_rate": 0.00011238095238095239,
      "loss": 0.1993,
      "step": 415
    },
    {
      "epoch": 6.672,
      "grad_norm": 0.7956132888793945,
      "learning_rate": 0.00011132275132275132,
      "loss": 0.1424,
      "step": 420
    },
    {
      "epoch": 6.752,
      "grad_norm": 0.5214114189147949,
      "learning_rate": 0.00011026455026455027,
      "loss": 0.2954,
      "step": 425
    },
    {
      "epoch": 6.832,
      "grad_norm": 0.451552152633667,
      "learning_rate": 0.0001092063492063492,
      "loss": 0.2379,
      "step": 430
    },
    {
      "epoch": 6.912,
      "grad_norm": 0.3915058374404907,
      "learning_rate": 0.00010814814814814815,
      "loss": 0.1679,
      "step": 435
    },
    {
      "epoch": 6.992,
      "grad_norm": 0.4908384680747986,
      "learning_rate": 0.00010708994708994708,
      "loss": 0.1818,
      "step": 440
    },
    {
      "epoch": 7.064,
      "grad_norm": 0.39697325229644775,
      "learning_rate": 0.00010603174603174604,
      "loss": 0.1874,
      "step": 445
    },
    {
      "epoch": 7.144,
      "grad_norm": 0.1201355829834938,
      "learning_rate": 0.00010497354497354497,
      "loss": 0.1307,
      "step": 450
    },
    {
      "epoch": 7.224,
      "grad_norm": 0.3046030104160309,
      "learning_rate": 0.00010391534391534393,
      "loss": 0.2191,
      "step": 455
    },
    {
      "epoch": 7.304,
      "grad_norm": 0.45002681016921997,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.2411,
      "step": 460
    },
    {
      "epoch": 7.384,
      "grad_norm": 0.5329495072364807,
      "learning_rate": 0.0001017989417989418,
      "loss": 0.1622,
      "step": 465
    },
    {
      "epoch": 7.464,
      "grad_norm": 0.2986811101436615,
      "learning_rate": 0.00010074074074074073,
      "loss": 0.1333,
      "step": 470
    },
    {
      "epoch": 7.5440000000000005,
      "grad_norm": 0.35804447531700134,
      "learning_rate": 9.968253968253969e-05,
      "loss": 0.1906,
      "step": 475
    },
    {
      "epoch": 7.624,
      "grad_norm": 0.47228243947029114,
      "learning_rate": 9.862433862433864e-05,
      "loss": 0.1576,
      "step": 480
    },
    {
      "epoch": 7.704,
      "grad_norm": 0.5396621823310852,
      "learning_rate": 9.756613756613757e-05,
      "loss": 0.2825,
      "step": 485
    },
    {
      "epoch": 7.784,
      "grad_norm": 0.5173963904380798,
      "learning_rate": 9.650793650793651e-05,
      "loss": 0.1449,
      "step": 490
    },
    {
      "epoch": 7.864,
      "grad_norm": 0.48154696822166443,
      "learning_rate": 9.544973544973545e-05,
      "loss": 0.1849,
      "step": 495
    },
    {
      "epoch": 7.944,
      "grad_norm": 0.5182950496673584,
      "learning_rate": 9.43915343915344e-05,
      "loss": 0.1502,
      "step": 500
    },
    {
      "epoch": 8.016,
      "grad_norm": 0.0861193984746933,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.064,
      "step": 505
    },
    {
      "epoch": 8.096,
      "grad_norm": 0.6597995162010193,
      "learning_rate": 9.227513227513229e-05,
      "loss": 0.1903,
      "step": 510
    },
    {
      "epoch": 8.176,
      "grad_norm": 0.5142132043838501,
      "learning_rate": 9.121693121693122e-05,
      "loss": 0.2149,
      "step": 515
    },
    {
      "epoch": 8.256,
      "grad_norm": 0.40375107526779175,
      "learning_rate": 9.015873015873016e-05,
      "loss": 0.1544,
      "step": 520
    },
    {
      "epoch": 8.336,
      "grad_norm": 0.5190391540527344,
      "learning_rate": 8.910052910052911e-05,
      "loss": 0.1522,
      "step": 525
    },
    {
      "epoch": 8.416,
      "grad_norm": 0.3834538459777832,
      "learning_rate": 8.804232804232805e-05,
      "loss": 0.1376,
      "step": 530
    },
    {
      "epoch": 8.496,
      "grad_norm": 0.09163343906402588,
      "learning_rate": 8.6984126984127e-05,
      "loss": 0.1358,
      "step": 535
    },
    {
      "epoch": 8.576,
      "grad_norm": 0.41458818316459656,
      "learning_rate": 8.592592592592593e-05,
      "loss": 0.1365,
      "step": 540
    },
    {
      "epoch": 8.656,
      "grad_norm": 0.7926501035690308,
      "learning_rate": 8.486772486772487e-05,
      "loss": 0.1767,
      "step": 545
    },
    {
      "epoch": 8.736,
      "grad_norm": 0.11168655008077621,
      "learning_rate": 8.380952380952382e-05,
      "loss": 0.1207,
      "step": 550
    },
    {
      "epoch": 8.816,
      "grad_norm": 0.7091956734657288,
      "learning_rate": 8.275132275132276e-05,
      "loss": 0.2785,
      "step": 555
    },
    {
      "epoch": 8.896,
      "grad_norm": 0.5166068077087402,
      "learning_rate": 8.16931216931217e-05,
      "loss": 0.1521,
      "step": 560
    },
    {
      "epoch": 8.975999999999999,
      "grad_norm": 0.11330940574407578,
      "learning_rate": 8.063492063492063e-05,
      "loss": 0.0896,
      "step": 565
    },
    {
      "epoch": 9.048,
      "grad_norm": 0.4172542989253998,
      "learning_rate": 7.957671957671958e-05,
      "loss": 0.1553,
      "step": 570
    },
    {
      "epoch": 9.128,
      "grad_norm": 0.3267105221748352,
      "learning_rate": 7.851851851851852e-05,
      "loss": 0.0988,
      "step": 575
    },
    {
      "epoch": 9.208,
      "grad_norm": 0.09925927221775055,
      "learning_rate": 7.746031746031747e-05,
      "loss": 0.1711,
      "step": 580
    },
    {
      "epoch": 9.288,
      "grad_norm": 0.10105247795581818,
      "learning_rate": 7.640211640211641e-05,
      "loss": 0.1178,
      "step": 585
    },
    {
      "epoch": 9.368,
      "grad_norm": 0.5346412658691406,
      "learning_rate": 7.534391534391536e-05,
      "loss": 0.1423,
      "step": 590
    },
    {
      "epoch": 9.448,
      "grad_norm": 0.3488040566444397,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.1944,
      "step": 595
    },
    {
      "epoch": 9.528,
      "grad_norm": 0.09770599752664566,
      "learning_rate": 7.322751322751323e-05,
      "loss": 0.1124,
      "step": 600
    },
    {
      "epoch": 9.608,
      "grad_norm": 0.37973159551620483,
      "learning_rate": 7.216931216931218e-05,
      "loss": 0.1172,
      "step": 605
    },
    {
      "epoch": 9.688,
      "grad_norm": 0.4093630909919739,
      "learning_rate": 7.111111111111112e-05,
      "loss": 0.2122,
      "step": 610
    },
    {
      "epoch": 9.768,
      "grad_norm": 0.5986504554748535,
      "learning_rate": 7.005291005291006e-05,
      "loss": 0.1108,
      "step": 615
    },
    {
      "epoch": 9.848,
      "grad_norm": 0.6888769865036011,
      "learning_rate": 6.8994708994709e-05,
      "loss": 0.171,
      "step": 620
    },
    {
      "epoch": 9.928,
      "grad_norm": 0.6886276006698608,
      "learning_rate": 6.793650793650794e-05,
      "loss": 0.1583,
      "step": 625
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.14647188782691956,
      "learning_rate": 6.687830687830688e-05,
      "loss": 0.1241,
      "step": 630
    },
    {
      "epoch": 10.08,
      "grad_norm": 0.49639692902565,
      "learning_rate": 6.582010582010583e-05,
      "loss": 0.0982,
      "step": 635
    },
    {
      "epoch": 10.16,
      "grad_norm": 0.39659595489501953,
      "learning_rate": 6.476190476190477e-05,
      "loss": 0.1528,
      "step": 640
    },
    {
      "epoch": 10.24,
      "grad_norm": 0.09441424906253815,
      "learning_rate": 6.37037037037037e-05,
      "loss": 0.0682,
      "step": 645
    },
    {
      "epoch": 10.32,
      "grad_norm": 0.6063386797904968,
      "learning_rate": 6.264550264550265e-05,
      "loss": 0.1675,
      "step": 650
    },
    {
      "epoch": 10.4,
      "grad_norm": 0.5919691324234009,
      "learning_rate": 6.158730158730159e-05,
      "loss": 0.1642,
      "step": 655
    },
    {
      "epoch": 10.48,
      "grad_norm": 0.7429628968238831,
      "learning_rate": 6.0529100529100536e-05,
      "loss": 0.2251,
      "step": 660
    },
    {
      "epoch": 10.56,
      "grad_norm": 0.10338687151670456,
      "learning_rate": 5.947089947089948e-05,
      "loss": 0.1107,
      "step": 665
    },
    {
      "epoch": 10.64,
      "grad_norm": 0.4300120770931244,
      "learning_rate": 5.841269841269842e-05,
      "loss": 0.1378,
      "step": 670
    },
    {
      "epoch": 10.72,
      "grad_norm": 0.7404960989952087,
      "learning_rate": 5.735449735449736e-05,
      "loss": 0.1149,
      "step": 675
    },
    {
      "epoch": 10.8,
      "grad_norm": 0.36336228251457214,
      "learning_rate": 5.62962962962963e-05,
      "loss": 0.1664,
      "step": 680
    },
    {
      "epoch": 10.88,
      "grad_norm": 0.08847704529762268,
      "learning_rate": 5.5238095238095244e-05,
      "loss": 0.0607,
      "step": 685
    },
    {
      "epoch": 10.96,
      "grad_norm": 0.5579912662506104,
      "learning_rate": 5.417989417989419e-05,
      "loss": 0.1199,
      "step": 690
    },
    {
      "epoch": 11.032,
      "grad_norm": 0.5667423605918884,
      "learning_rate": 5.3121693121693126e-05,
      "loss": 0.1978,
      "step": 695
    },
    {
      "epoch": 11.112,
      "grad_norm": 0.4077160060405731,
      "learning_rate": 5.206349206349207e-05,
      "loss": 0.1653,
      "step": 700
    },
    {
      "epoch": 11.192,
      "grad_norm": 0.544478714466095,
      "learning_rate": 5.1005291005291015e-05,
      "loss": 0.1229,
      "step": 705
    },
    {
      "epoch": 11.272,
      "grad_norm": 0.3783157169818878,
      "learning_rate": 4.9947089947089946e-05,
      "loss": 0.157,
      "step": 710
    },
    {
      "epoch": 11.352,
      "grad_norm": 0.42868077754974365,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.1476,
      "step": 715
    },
    {
      "epoch": 11.432,
      "grad_norm": 0.36235034465789795,
      "learning_rate": 4.7830687830687834e-05,
      "loss": 0.1538,
      "step": 720
    },
    {
      "epoch": 11.512,
      "grad_norm": 0.4006042182445526,
      "learning_rate": 4.677248677248677e-05,
      "loss": 0.1085,
      "step": 725
    },
    {
      "epoch": 11.592,
      "grad_norm": 0.5774800777435303,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.1797,
      "step": 730
    },
    {
      "epoch": 11.672,
      "grad_norm": 0.5384610891342163,
      "learning_rate": 4.4656084656084654e-05,
      "loss": 0.0827,
      "step": 735
    },
    {
      "epoch": 11.752,
      "grad_norm": 0.1593407839536667,
      "learning_rate": 4.35978835978836e-05,
      "loss": 0.0295,
      "step": 740
    },
    {
      "epoch": 11.832,
      "grad_norm": 0.44579586386680603,
      "learning_rate": 4.253968253968254e-05,
      "loss": 0.1155,
      "step": 745
    },
    {
      "epoch": 11.912,
      "grad_norm": 0.5219768285751343,
      "learning_rate": 4.148148148148148e-05,
      "loss": 0.0847,
      "step": 750
    },
    {
      "epoch": 11.992,
      "grad_norm": 0.8660899996757507,
      "learning_rate": 4.0423280423280424e-05,
      "loss": 0.1725,
      "step": 755
    },
    {
      "epoch": 12.064,
      "grad_norm": 0.11769631505012512,
      "learning_rate": 3.936507936507937e-05,
      "loss": 0.0811,
      "step": 760
    },
    {
      "epoch": 12.144,
      "grad_norm": 0.4868147671222687,
      "learning_rate": 3.8306878306878306e-05,
      "loss": 0.0962,
      "step": 765
    },
    {
      "epoch": 12.224,
      "grad_norm": 0.9700756669044495,
      "learning_rate": 3.724867724867725e-05,
      "loss": 0.1994,
      "step": 770
    },
    {
      "epoch": 12.304,
      "grad_norm": 0.37592780590057373,
      "learning_rate": 3.619047619047619e-05,
      "loss": 0.0733,
      "step": 775
    },
    {
      "epoch": 12.384,
      "grad_norm": 0.5867070555686951,
      "learning_rate": 3.513227513227513e-05,
      "loss": 0.1357,
      "step": 780
    },
    {
      "epoch": 12.464,
      "grad_norm": 0.8047243356704712,
      "learning_rate": 3.4074074074074077e-05,
      "loss": 0.1344,
      "step": 785
    },
    {
      "epoch": 12.544,
      "grad_norm": 0.5029036998748779,
      "learning_rate": 3.3015873015873014e-05,
      "loss": 0.1436,
      "step": 790
    },
    {
      "epoch": 12.624,
      "grad_norm": 0.11095529794692993,
      "learning_rate": 3.195767195767196e-05,
      "loss": 0.0743,
      "step": 795
    },
    {
      "epoch": 12.704,
      "grad_norm": 0.7611116766929626,
      "learning_rate": 3.08994708994709e-05,
      "loss": 0.1678,
      "step": 800
    },
    {
      "epoch": 12.784,
      "grad_norm": 0.3531305193901062,
      "learning_rate": 2.9841269841269844e-05,
      "loss": 0.0689,
      "step": 805
    },
    {
      "epoch": 12.864,
      "grad_norm": 0.6520860195159912,
      "learning_rate": 2.8783068783068785e-05,
      "loss": 0.1083,
      "step": 810
    },
    {
      "epoch": 12.943999999999999,
      "grad_norm": 0.7268972396850586,
      "learning_rate": 2.7724867724867726e-05,
      "loss": 0.1022,
      "step": 815
    },
    {
      "epoch": 13.016,
      "grad_norm": 0.45361563563346863,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.1598,
      "step": 820
    },
    {
      "epoch": 13.096,
      "grad_norm": 0.4314562976360321,
      "learning_rate": 2.560846560846561e-05,
      "loss": 0.1082,
      "step": 825
    },
    {
      "epoch": 13.176,
      "grad_norm": 0.6842070817947388,
      "learning_rate": 2.4550264550264552e-05,
      "loss": 0.124,
      "step": 830
    },
    {
      "epoch": 13.256,
      "grad_norm": 0.39738884568214417,
      "learning_rate": 2.3492063492063493e-05,
      "loss": 0.1254,
      "step": 835
    },
    {
      "epoch": 13.336,
      "grad_norm": 0.12071508914232254,
      "learning_rate": 2.2433862433862434e-05,
      "loss": 0.1152,
      "step": 840
    },
    {
      "epoch": 13.416,
      "grad_norm": 0.5674815773963928,
      "learning_rate": 2.1375661375661378e-05,
      "loss": 0.124,
      "step": 845
    },
    {
      "epoch": 13.496,
      "grad_norm": 0.5919864177703857,
      "learning_rate": 2.031746031746032e-05,
      "loss": 0.1179,
      "step": 850
    },
    {
      "epoch": 13.576,
      "grad_norm": 0.6215223670005798,
      "learning_rate": 1.925925925925926e-05,
      "loss": 0.1065,
      "step": 855
    },
    {
      "epoch": 13.656,
      "grad_norm": 0.3862781524658203,
      "learning_rate": 1.82010582010582e-05,
      "loss": 0.1322,
      "step": 860
    },
    {
      "epoch": 13.736,
      "grad_norm": 0.6149358153343201,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 0.1332,
      "step": 865
    },
    {
      "epoch": 13.816,
      "grad_norm": 0.10494565218687057,
      "learning_rate": 1.6084656084656086e-05,
      "loss": 0.0709,
      "step": 870
    },
    {
      "epoch": 13.896,
      "grad_norm": 0.3413187861442566,
      "learning_rate": 1.5026455026455027e-05,
      "loss": 0.0844,
      "step": 875
    },
    {
      "epoch": 13.975999999999999,
      "grad_norm": 0.9283654093742371,
      "learning_rate": 1.396825396825397e-05,
      "loss": 0.1242,
      "step": 880
    },
    {
      "epoch": 14.048,
      "grad_norm": 0.414500892162323,
      "learning_rate": 1.291005291005291e-05,
      "loss": 0.0636,
      "step": 885
    },
    {
      "epoch": 14.128,
      "grad_norm": 0.7980809807777405,
      "learning_rate": 1.1851851851851853e-05,
      "loss": 0.1406,
      "step": 890
    },
    {
      "epoch": 14.208,
      "grad_norm": 0.578342854976654,
      "learning_rate": 1.0793650793650794e-05,
      "loss": 0.0871,
      "step": 895
    },
    {
      "epoch": 14.288,
      "grad_norm": 0.534412145614624,
      "learning_rate": 9.735449735449737e-06,
      "loss": 0.1519,
      "step": 900
    },
    {
      "epoch": 14.368,
      "grad_norm": 0.5892877578735352,
      "learning_rate": 8.677248677248678e-06,
      "loss": 0.1266,
      "step": 905
    },
    {
      "epoch": 14.448,
      "grad_norm": 0.37257716059684753,
      "learning_rate": 7.6190476190476205e-06,
      "loss": 0.0391,
      "step": 910
    },
    {
      "epoch": 14.528,
      "grad_norm": 0.3907882273197174,
      "learning_rate": 6.5608465608465606e-06,
      "loss": 0.1389,
      "step": 915
    },
    {
      "epoch": 14.608,
      "grad_norm": 0.37209758162498474,
      "learning_rate": 5.502645502645503e-06,
      "loss": 0.1241,
      "step": 920
    },
    {
      "epoch": 14.688,
      "grad_norm": 0.8011608123779297,
      "learning_rate": 4.444444444444445e-06,
      "loss": 0.114,
      "step": 925
    },
    {
      "epoch": 14.768,
      "grad_norm": 0.7566561698913574,
      "learning_rate": 3.3862433862433868e-06,
      "loss": 0.1256,
      "step": 930
    },
    {
      "epoch": 14.848,
      "grad_norm": 0.3568294048309326,
      "learning_rate": 2.328042328042328e-06,
      "loss": 0.0965,
      "step": 935
    },
    {
      "epoch": 14.928,
      "grad_norm": 0.5089025497436523,
      "learning_rate": 1.26984126984127e-06,
      "loss": 0.0677,
      "step": 940
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.14187757670879364,
      "learning_rate": 2.1164021164021167e-07,
      "loss": 0.1032,
      "step": 945
    }
  ],
  "logging_steps": 5,
  "max_steps": 945,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 15,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 6.1265608704e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
